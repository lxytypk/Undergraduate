# Inspired by https://keon.io/deep-q-learning/

import random
import gym
import math
import numpy as np
import tensorflow as tf
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

class DQNCartPoleSolver():
    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):
        self.memory = deque(maxlen=100000) #è®°å¿†ä½“ä½¿ç”¨é˜Ÿåˆ—å®ç°ï¼Œé˜Ÿåˆ—æ»¡åæ ¹æ®æ’å…¥é¡ºåºè‡ªåŠ¨åˆ é™¤è€æ•°æ®
        self.env = gym.make('CartPole-v0') #ç¯å¢ƒ
        if monitor: self.env = gym.wrappers.Monitor(self.env, './data/cartpole-1', force=True)
        self.gamma = gamma #æŠ˜æ‰£å› å­
        self.epsilon = epsilon #æ¢ç´¢ç‡
        self.epsilon_min = epsilon_min #æ¢ç´¢ç‡è¡°å‡åˆ°æ­¤å€¼ä»¥ä¸‹æ—¶åœæ­¢
        self.epsilon_decay = epsilon_log_decay #æ¢ç´¢ç‡è¡°å‡å› å­
        self.alpha = alpha
        self.alpha_decay = alpha_decay #å­¦ä¹ ç‡è¡°å‡å› å­
        self.n_episodes = n_episodes #è®­ç»ƒçš„æ€»æ¬¡æ•°
        self.n_win_ticks = n_win_ticks #è¿ç»­èƒœåˆ©çš„æ¬¡æ•°
        self.batch_size = batch_size #æ¯æ¬¡è®­ç»ƒæ—¶çš„æ ·æœ¬æ‰¹å¤§å°
        self.quiet = quiet
        self.filepath="model-epoch-{}.hdf5" #æ¨¡å‹ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps

        #Neural Net for Deep-Q learning Model
        self.model = Sequential()
        self.model.add(Dense(24, input_dim=4, activation='tanh'))
        self.model.add(Dense(48, activation='tanh'))
        self.model.add(Dense(2, activation='linear'))
        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha, decay=self.alpha_decay)) # æŒ‡å®šæŸå¤±å‡½æ•°ä»¥åŠä¼˜åŒ–å™¨

    # åœ¨è®°å¿†ä½“ï¼ˆç»éªŒå›æ”¾æ± ï¼‰ä¸­ä¿å­˜å…·ä½“æŸä¸€æ—¶åˆ»çš„å½“å‰çŠ¶æ€ä¿¡æ¯
    #å­˜å‚¨åœ¨memory
    def remember(self, state, action, reward, next_state, done):
        #å½“å‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ã€æ˜¯å¦ç»“æŸ
        self.memory.append((state, action, reward, next_state, done))

    #æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
    #åœ¨è®­ç»ƒåˆæœŸï¼Œæ¢ç´¢ç‡è¾ƒé«˜ï¼Œagentæ›´å¤šåœ°è¿›è¡Œæ¢ç´¢ï¼›è€Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¢ç´¢ç‡é€æ¸å‡å°ï¼Œagentæ›´å¤šåœ°åˆ©ç”¨å·²çŸ¥ä¿¡æ¯
    def choose_action(self, state, epsilon):
        '''è´ªå©ªç­–ç•¥'''
        #å¦‚æœä¸€ä¸ªéšæœºæ•°å°äºç­‰äºæ¢ç´¢ç‡epsilonï¼Œåˆ™éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œå³ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œã€‚
        #å¦åˆ™ï¼Œä½¿ç”¨å½“å‰ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆself.modelï¼‰å¯¹å½“å‰çŠ¶æ€è¿›è¡Œé¢„æµ‹ï¼Œç„¶åé€‰æ‹©é¢„æµ‹å€¼æœ€å¤§çš„åŠ¨ä½œä½œä¸ºè¾“å‡ºã€‚è¿™æ˜¯ä¸€ç§åŸºäºè´ªå©ªç­–ç•¥ï¼ˆgreedy policyï¼‰çš„è¡Œä¸ºï¼Œå³é€‰æ‹©å¯¹åº”é¢„æµ‹å€¼æœ€å¤§çš„åŠ¨ä½œï¼Œä»¥æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±
        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))

    #è°ƒæ•´æ¢ç´¢ç‡
    def get_epsilon(self, t):
        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))

    #å¯¹çŠ¶æ€è¿›è¡Œé¢„å¤„ç†
    #å°†åŸå§‹çŠ¶æ€è½¬æ¢æˆç¥ç»ç½‘ç»œèƒ½å¤Ÿæ¥å—çš„è¾“å…¥æ ¼å¼
    def preprocess_state(self, state):
        return np.reshape(state, [1, 4])

    #æ‰§è¡Œç»éªŒå›æ”¾
    #batch_sizeæ˜¯æ¯æ¬¡ä»ç»éªŒæ± ä¸­æŠ½å–çš„æ ·æœ¬æ•°é‡
    def replay(self, batch_size):
        #å­˜å‚¨è¾“å…¥å’Œç›®æ ‡è¾“å‡º
        x_batch, y_batch = [], []
        #ä»ç»éªŒæ± ï¼ˆself.memoryï¼‰ä¸­éšæœºæŠ½å–çš„ä¸€æ‰¹æ ·æœ¬ï¼Œæ•°é‡ä¸ºbatch_sizeå’Œç»éªŒæ± å¤§å°çš„è¾ƒå°å€¼
        minibatch = random.sample(
            self.memory, min(len(self.memory), batch_size))
        #å¯¹äºæŠ½å–çš„æ¯ä¸€ä¸ªæ ·æœ¬æ“ä½œ
        for state, action, reward, next_state, done in minibatch:
            #ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹å½“å‰çŠ¶æ€ï¼ˆstateï¼‰è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°é¢„æµ‹å€¼
            y_target = self.model.predict(state)
            #æ ¹æ®å½“å‰çš„å¥–åŠ±ï¼ˆrewardï¼‰ã€æ˜¯å¦ç»ˆæ­¢ï¼ˆdoneï¼‰ä»¥åŠä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ˆnext_stateï¼‰çš„æœ€å¤§é¢„æµ‹å€¼ï¼Œæ›´æ–°ç›®æ ‡å€¼ä¸­å¯¹åº”åŠ¨ä½œçš„é¢„æµ‹å€¼
            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])
            x_batch.append(state[0])
            y_batch.append(y_target[0])

        #è®­ç»ƒï¼Œæ›´æ–°ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°
        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)
        #å½“å‰çš„æ¢ç´¢ç‡epsilonå¤§äºæœ€å°æ¢ç´¢ç‡epsilon_minï¼Œåˆ™å°†æ¢ç´¢ç‡è¿›è¡Œè¡°å‡ï¼Œä»¥æ§åˆ¶æ¢ç´¢ç‡éšè®­ç»ƒè¿›è¡Œé€æ¸å‡å°
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def run(self):
        scores = deque(maxlen=100)

        for e in range(self.n_episodes):
            #å°†ç¯å¢ƒé‡ç½®ä¸ºåˆå§‹çŠ¶æ€ï¼Œå¹¶å¯¹çŠ¶æ€è¿›è¡Œé¢„å¤„ç†
            state = self.preprocess_state(self.env.reset())
            done = False
            i = 0
            while not done:
                #æ ¹æ®å½“å‰çŠ¶æ€å’Œä¸€ä¸ªéšè®­ç»ƒæ¬¡æ•°å˜åŒ–çš„Îµå€¼é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
                action = self.choose_action(state, self.get_epsilon(e))
                #æ‰§è¡Œé€‰å®šçš„åŠ¨ä½œï¼Œå¹¶è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ä»¥åŠæ˜¯å¦å®Œæˆçš„æ ‡å¿—
                next_state, reward, done, _ = self.env.step(action)
                next_state = self.preprocess_state(next_state)
                #å°†ç»éªŒå­˜å‚¨åˆ°è®°å¿†ç¼“å†²åŒºä¸­
                self.remember(state, action, reward, next_state, done)
                #æ›´æ–°å½“å‰çŠ¶æ€
                state = next_state
                #è¯¥è½®æ¬¡çš„æ­¥æ•°
                i += 1

            scores.append(i)
            #è®¡ç®—æœ€è¿‘100ä¸ªè½®æ¬¡çš„å¹³å‡å¾—åˆ†
            mean_score = np.mean(scores)
            #å¦‚æœå¹³å‡å¾—åˆ†è¶…è¿‡æŒ‡å®šçš„é˜ˆå€¼ï¼Œå¹¶ä¸”å·²ç»è¿è¡Œäº†è¶³å¤Ÿå¤šçš„è½®æ¬¡ï¼Œåˆ™ä»»åŠ¡è¢«è§†ä¸ºå·²è§£å†³
            if mean_score >= self.n_win_ticks and e >= 100:
                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))
                return e - 100
            if e % 100 == 0 and not self.quiet:
                self.model.save(self.filepath.format(e))
                print('[Episode {}] - Mean survival time {}'.format(e, mean_score))

            #ä½¿ç”¨è®°å¿†ç¼“å†²åŒºä¸­çš„ç»éªŒè¿›è¡Œè®­ç»ƒ
            self.replay(self.batch_size)

        if not self.quiet: print('Did not solve after {} episodes ğŸ˜'.format(e))
        return e

if __name__ == '__main__':
    agent = DQNCartPoleSolver()
    agent.run()