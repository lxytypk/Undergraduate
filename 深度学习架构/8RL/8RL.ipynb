{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第八章 强化学习\n",
    "* 使用参考代码完成第5节中DQN训练CartPole-v0游戏的训练过程，得到最佳模型权重文件。\n",
    "* 自己编写代码，实现游戏的验证过程，显示游戏图片，得到一个分数超过100的结果，结果以截图和数据形式给出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.玩一个 Atari 游戏\n",
    "* 使用DQN训练游戏CartPole-v0，原理见Q-Learning算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000) #记忆体使用队列实现，队列满后根据插入顺序自动删除老数据\n",
    "        self.env = gym.make('CartPole-v0') #环境\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, './data/cartpole-1', force=True)\n",
    "        self.gamma = gamma #折扣因子\n",
    "        self.epsilon = epsilon #探索率\n",
    "        self.epsilon_min = epsilon_min #探索率衰减到此值以下时停止\n",
    "        self.epsilon_decay = epsilon_log_decay #探索率衰减因子\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay #学习率衰减因子\n",
    "        self.n_episodes = n_episodes #训练的总次数\n",
    "        self.n_win_ticks = n_win_ticks #连续胜利的次数\n",
    "        self.batch_size = batch_size #每次训练时的样本批大小\n",
    "        self.quiet = quiet\n",
    "        self.filepath=\"model-epoch-{}.hdf5\" #模型保存的文件路径\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        #Neural Net for Deep-Q learning Model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=4, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha, decay=self.alpha_decay)) # 指定损失函数以及优化器\n",
    "\n",
    "    # 在记忆体（经验回放池）中保存具体某一时刻的当前状态信息\n",
    "    #存储在memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #当前状态、动作、奖励、下一个状态、是否结束\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #根据当前状态选择一个动作\n",
    "    #在训练初期，探索率较高，agent更多地进行探索；而随着训练的进行，探索率逐渐减小，agent更多地利用已知信息\n",
    "    def choose_action(self, state, epsilon):\n",
    "        '''贪婪策略'''\n",
    "        #如果一个随机数小于等于探索率epsilon，则随机选择一个动作，即从动作空间中随机采样一个动作。\n",
    "        #否则，使用当前神经网络模型（self.model）对当前状态进行预测，然后选择预测值最大的动作作为输出。这是一种基于贪婪策略（greedy policy）的行为，即选择对应预测值最大的动作，以最大化预期奖励\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    #调整探索率\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    #对状态进行预处理\n",
    "    #将原始状态转换成神经网络能够接受的输入格式\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    #执行经验回放\n",
    "    #batch_size是每次从经验池中抽取的样本数量\n",
    "    def replay(self, batch_size):\n",
    "        #存储输入和目标输出\n",
    "        x_batch, y_batch = [], []\n",
    "        #从经验池（self.memory）中随机抽取的一批样本，数量为batch_size和经验池大小的较小值\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        #对于抽取的每一个样本操作\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #神经网络模型对当前状态（state）进行预测，得到预测值\n",
    "            y_target = self.model.predict(state)\n",
    "            #根据当前的奖励（reward）、是否终止（done）以及下一个状态（next_state）的最大预测值，更新目标值中对应动作的预测值\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        #训练，更新神经网络模型的参数\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        #当前的探索率epsilon大于最小探索率epsilon_min，则将探索率进行衰减，以控制探索率随训练进行逐渐减小\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            #将环境重置为初始状态，并对状态进行预处理\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #根据当前状态和一个随训练次数变化的ε值选择一个动作\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                #执行选定的动作，并获取下一个状态、奖励以及是否完成的标志\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                #将经验存储到记忆缓冲区中\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                #更新当前状态\n",
    "                state = next_state\n",
    "                #该轮次的步数\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            #计算最近100个轮次的平均得分\n",
    "            mean_score = np.mean(scores)\n",
    "            #如果平均得分超过指定的阈值，并且已经运行了足够多的轮次，则任务被视为已解决\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                self.model.save(self.filepath.format(e))\n",
    "                print('[Episode {}] - Mean survival time {}'.format(e, mean_score))\n",
    "\n",
    "            #使用记忆缓冲区中的经验进行训练\n",
    "            self.replay(self.batch_size)\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 166.0\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the trained model\n",
    "# model = DQNCartPoleSolver().model\n",
    "# model.load_weights(\"model-epoch-400.hdf5\")\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "\n",
    "# # Run the game until a score greater than 100 is achieved\n",
    "# done = False\n",
    "# state = env.reset()\n",
    "# score = 0\n",
    "# while not done:\n",
    "#     # Render the game image\n",
    "#     env.render()\n",
    "    \n",
    "#     # Choose an action based on the trained model\n",
    "#     state = np.reshape(state, [1, 4])\n",
    "#     action = np.argmax(model.predict(state)[0])\n",
    "    \n",
    "#     # Take the chosen action\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "#     # Update the score\n",
    "#     score += reward\n",
    "    \n",
    "#     # Update the current state\n",
    "#     state = next_state\n",
    "\n",
    "# # Close the environment\n",
    "# env.close()\n",
    "\n",
    "# # Display the final score\n",
    "# print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 192.0\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the trained model\n",
    "# model = DQNCartPoleSolver().model\n",
    "# model.load_weights(\"model-epoch-500.hdf5\")\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "\n",
    "# # Run the game until a score greater than 100 is achieved\n",
    "# done = False\n",
    "# state = env.reset()\n",
    "# score = 0\n",
    "# while not done:\n",
    "#     # Render the game image\n",
    "#     env.render()\n",
    "    \n",
    "#     # Choose an action based on the trained model\n",
    "#     state = np.reshape(state, [1, 4])\n",
    "#     action = np.argmax(model.predict(state)[0])\n",
    "    \n",
    "#     # Take the chosen action\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "#     # Update the score\n",
    "#     score += reward\n",
    "    \n",
    "#     # Update the current state\n",
    "#     state = next_state\n",
    "\n",
    "# # Close the environment\n",
    "# env.close()\n",
    "\n",
    "# # Display the final score\n",
    "# print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python36\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 无法在设置线程模式后对其加以更改。\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 198.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "model = DQNCartPoleSolver().model\n",
    "model.load_weights(\"model-epoch-500.hdf5\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Run the game until a score greater than 100 is achieved\n",
    "done = False\n",
    "state = env.reset()\n",
    "score = 0\n",
    "\n",
    "# 创建一个空的图像列表\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    # Render the game image\n",
    "    env.render()\n",
    "    \n",
    "    # 将当前游戏图像添加到列表中\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "    # Choose an action based on the trained model\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    \n",
    "    # Take the chosen action\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Update the score\n",
    "    score += reward\n",
    "    \n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "    \n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Display the final score\n",
    "print(\"Final Score:\", score)\n",
    "\n",
    "# 保存游戏过程的图像\n",
    "for i, frame in enumerate(frames):\n",
    "    import imageio\n",
    "\n",
    "    # 保存游戏过程的图像为GIF动画\n",
    "    imageio.mimsave('game_animation.gif', frames, fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
