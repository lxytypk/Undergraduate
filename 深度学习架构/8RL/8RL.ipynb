{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬å…«ç«  å¼ºåŒ–å­¦ä¹ \n",
    "* ä½¿ç”¨å‚è€ƒä»£ç å®Œæˆç¬¬5èŠ‚ä¸­DQNè®­ç»ƒCartPole-v0æ¸¸æˆçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¾—åˆ°æœ€ä½³æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚\n",
    "* è‡ªå·±ç¼–å†™ä»£ç ï¼Œå®ç°æ¸¸æˆçš„éªŒè¯è¿‡ç¨‹ï¼Œæ˜¾ç¤ºæ¸¸æˆå›¾ç‰‡ï¼Œå¾—åˆ°ä¸€ä¸ªåˆ†æ•°è¶…è¿‡100çš„ç»“æœï¼Œç»“æœä»¥æˆªå›¾å’Œæ•°æ®å½¢å¼ç»™å‡ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.ç©ä¸€ä¸ª Atari æ¸¸æˆ\n",
    "* ä½¿ç”¨DQNè®­ç»ƒæ¸¸æˆCartPole-v0ï¼ŒåŸç†è§Q-Learningç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000) #è®°å¿†ä½“ä½¿ç”¨é˜Ÿåˆ—å®ç°ï¼Œé˜Ÿåˆ—æ»¡åæ ¹æ®æ’å…¥é¡ºåºè‡ªåŠ¨åˆ é™¤è€æ•°æ®\n",
    "        self.env = gym.make('CartPole-v0') #ç¯å¢ƒ\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, './data/cartpole-1', force=True)\n",
    "        self.gamma = gamma #æŠ˜æ‰£å› å­\n",
    "        self.epsilon = epsilon #æ¢ç´¢ç‡\n",
    "        self.epsilon_min = epsilon_min #æ¢ç´¢ç‡è¡°å‡åˆ°æ­¤å€¼ä»¥ä¸‹æ—¶åœæ­¢\n",
    "        self.epsilon_decay = epsilon_log_decay #æ¢ç´¢ç‡è¡°å‡å› å­\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay #å­¦ä¹ ç‡è¡°å‡å› å­\n",
    "        self.n_episodes = n_episodes #è®­ç»ƒçš„æ€»æ¬¡æ•°\n",
    "        self.n_win_ticks = n_win_ticks #è¿ç»­èƒœåˆ©çš„æ¬¡æ•°\n",
    "        self.batch_size = batch_size #æ¯æ¬¡è®­ç»ƒæ—¶çš„æ ·æœ¬æ‰¹å¤§å°\n",
    "        self.quiet = quiet\n",
    "        self.filepath=\"model-epoch-{}.hdf5\" #æ¨¡å‹ä¿å­˜çš„æ–‡ä»¶è·¯å¾„\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        #Neural Net for Deep-Q learning Model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=4, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.alpha, decay=self.alpha_decay)) # æŒ‡å®šæŸå¤±å‡½æ•°ä»¥åŠä¼˜åŒ–å™¨\n",
    "\n",
    "    # åœ¨è®°å¿†ä½“ï¼ˆç»éªŒå›æ”¾æ± ï¼‰ä¸­ä¿å­˜å…·ä½“æŸä¸€æ—¶åˆ»çš„å½“å‰çŠ¶æ€ä¿¡æ¯\n",
    "    #å­˜å‚¨åœ¨memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #å½“å‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ã€æ˜¯å¦ç»“æŸ\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ\n",
    "    #åœ¨è®­ç»ƒåˆæœŸï¼Œæ¢ç´¢ç‡è¾ƒé«˜ï¼Œagentæ›´å¤šåœ°è¿›è¡Œæ¢ç´¢ï¼›è€Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¢ç´¢ç‡é€æ¸å‡å°ï¼Œagentæ›´å¤šåœ°åˆ©ç”¨å·²çŸ¥ä¿¡æ¯\n",
    "    def choose_action(self, state, epsilon):\n",
    "        '''è´ªå©ªç­–ç•¥'''\n",
    "        #å¦‚æœä¸€ä¸ªéšæœºæ•°å°äºç­‰äºæ¢ç´¢ç‡epsilonï¼Œåˆ™éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œå³ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œã€‚\n",
    "        #å¦åˆ™ï¼Œä½¿ç”¨å½“å‰ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆself.modelï¼‰å¯¹å½“å‰çŠ¶æ€è¿›è¡Œé¢„æµ‹ï¼Œç„¶åé€‰æ‹©é¢„æµ‹å€¼æœ€å¤§çš„åŠ¨ä½œä½œä¸ºè¾“å‡ºã€‚è¿™æ˜¯ä¸€ç§åŸºäºè´ªå©ªç­–ç•¥ï¼ˆgreedy policyï¼‰çš„è¡Œä¸ºï¼Œå³é€‰æ‹©å¯¹åº”é¢„æµ‹å€¼æœ€å¤§çš„åŠ¨ä½œï¼Œä»¥æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    #è°ƒæ•´æ¢ç´¢ç‡\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    #å¯¹çŠ¶æ€è¿›è¡Œé¢„å¤„ç†\n",
    "    #å°†åŸå§‹çŠ¶æ€è½¬æ¢æˆç¥ç»ç½‘ç»œèƒ½å¤Ÿæ¥å—çš„è¾“å…¥æ ¼å¼\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    #æ‰§è¡Œç»éªŒå›æ”¾\n",
    "    #batch_sizeæ˜¯æ¯æ¬¡ä»ç»éªŒæ± ä¸­æŠ½å–çš„æ ·æœ¬æ•°é‡\n",
    "    def replay(self, batch_size):\n",
    "        #å­˜å‚¨è¾“å…¥å’Œç›®æ ‡è¾“å‡º\n",
    "        x_batch, y_batch = [], []\n",
    "        #ä»ç»éªŒæ± ï¼ˆself.memoryï¼‰ä¸­éšæœºæŠ½å–çš„ä¸€æ‰¹æ ·æœ¬ï¼Œæ•°é‡ä¸ºbatch_sizeå’Œç»éªŒæ± å¤§å°çš„è¾ƒå°å€¼\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        #å¯¹äºæŠ½å–çš„æ¯ä¸€ä¸ªæ ·æœ¬æ“ä½œ\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹å½“å‰çŠ¶æ€ï¼ˆstateï¼‰è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°é¢„æµ‹å€¼\n",
    "            y_target = self.model.predict(state)\n",
    "            #æ ¹æ®å½“å‰çš„å¥–åŠ±ï¼ˆrewardï¼‰ã€æ˜¯å¦ç»ˆæ­¢ï¼ˆdoneï¼‰ä»¥åŠä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ˆnext_stateï¼‰çš„æœ€å¤§é¢„æµ‹å€¼ï¼Œæ›´æ–°ç›®æ ‡å€¼ä¸­å¯¹åº”åŠ¨ä½œçš„é¢„æµ‹å€¼\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "\n",
    "        #è®­ç»ƒï¼Œæ›´æ–°ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°\n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        #å½“å‰çš„æ¢ç´¢ç‡epsilonå¤§äºæœ€å°æ¢ç´¢ç‡epsilon_minï¼Œåˆ™å°†æ¢ç´¢ç‡è¿›è¡Œè¡°å‡ï¼Œä»¥æ§åˆ¶æ¢ç´¢ç‡éšè®­ç»ƒè¿›è¡Œé€æ¸å‡å°\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            #å°†ç¯å¢ƒé‡ç½®ä¸ºåˆå§‹çŠ¶æ€ï¼Œå¹¶å¯¹çŠ¶æ€è¿›è¡Œé¢„å¤„ç†\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #æ ¹æ®å½“å‰çŠ¶æ€å’Œä¸€ä¸ªéšè®­ç»ƒæ¬¡æ•°å˜åŒ–çš„Îµå€¼é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                #æ‰§è¡Œé€‰å®šçš„åŠ¨ä½œï¼Œå¹¶è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ä»¥åŠæ˜¯å¦å®Œæˆçš„æ ‡å¿—\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                #å°†ç»éªŒå­˜å‚¨åˆ°è®°å¿†ç¼“å†²åŒºä¸­\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                #æ›´æ–°å½“å‰çŠ¶æ€\n",
    "                state = next_state\n",
    "                #è¯¥è½®æ¬¡çš„æ­¥æ•°\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            #è®¡ç®—æœ€è¿‘100ä¸ªè½®æ¬¡çš„å¹³å‡å¾—åˆ†\n",
    "            mean_score = np.mean(scores)\n",
    "            #å¦‚æœå¹³å‡å¾—åˆ†è¶…è¿‡æŒ‡å®šçš„é˜ˆå€¼ï¼Œå¹¶ä¸”å·²ç»è¿è¡Œäº†è¶³å¤Ÿå¤šçš„è½®æ¬¡ï¼Œåˆ™ä»»åŠ¡è¢«è§†ä¸ºå·²è§£å†³\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                self.model.save(self.filepath.format(e))\n",
    "                print('[Episode {}] - Mean survival time {}'.format(e, mean_score))\n",
    "\n",
    "            #ä½¿ç”¨è®°å¿†ç¼“å†²åŒºä¸­çš„ç»éªŒè¿›è¡Œè®­ç»ƒ\n",
    "            self.replay(self.batch_size)\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes ğŸ˜'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 166.0\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the trained model\n",
    "# model = DQNCartPoleSolver().model\n",
    "# model.load_weights(\"model-epoch-400.hdf5\")\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "\n",
    "# # Run the game until a score greater than 100 is achieved\n",
    "# done = False\n",
    "# state = env.reset()\n",
    "# score = 0\n",
    "# while not done:\n",
    "#     # Render the game image\n",
    "#     env.render()\n",
    "    \n",
    "#     # Choose an action based on the trained model\n",
    "#     state = np.reshape(state, [1, 4])\n",
    "#     action = np.argmax(model.predict(state)[0])\n",
    "    \n",
    "#     # Take the chosen action\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "#     # Update the score\n",
    "#     score += reward\n",
    "    \n",
    "#     # Update the current state\n",
    "#     state = next_state\n",
    "\n",
    "# # Close the environment\n",
    "# env.close()\n",
    "\n",
    "# # Display the final score\n",
    "# print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 192.0\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the trained model\n",
    "# model = DQNCartPoleSolver().model\n",
    "# model.load_weights(\"model-epoch-500.hdf5\")\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "\n",
    "# # Run the game until a score greater than 100 is achieved\n",
    "# done = False\n",
    "# state = env.reset()\n",
    "# score = 0\n",
    "# while not done:\n",
    "#     # Render the game image\n",
    "#     env.render()\n",
    "    \n",
    "#     # Choose an action based on the trained model\n",
    "#     state = np.reshape(state, [1, 4])\n",
    "#     action = np.argmax(model.predict(state)[0])\n",
    "    \n",
    "#     # Take the chosen action\n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "#     # Update the score\n",
    "#     score += reward\n",
    "    \n",
    "#     # Update the current state\n",
    "#     state = next_state\n",
    "\n",
    "# # Close the environment\n",
    "# env.close()\n",
    "\n",
    "# # Display the final score\n",
    "# print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python36\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] æ— æ³•åœ¨è®¾ç½®çº¿ç¨‹æ¨¡å¼åå¯¹å…¶åŠ ä»¥æ›´æ”¹ã€‚\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 198.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "model = DQNCartPoleSolver().model\n",
    "model.load_weights(\"model-epoch-500.hdf5\")\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Run the game until a score greater than 100 is achieved\n",
    "done = False\n",
    "state = env.reset()\n",
    "score = 0\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç©ºçš„å›¾åƒåˆ—è¡¨\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    # Render the game image\n",
    "    env.render()\n",
    "    \n",
    "    # å°†å½“å‰æ¸¸æˆå›¾åƒæ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "\n",
    "    # Choose an action based on the trained model\n",
    "    state = np.reshape(state, [1, 4])\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    \n",
    "    # Take the chosen action\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Update the score\n",
    "    score += reward\n",
    "    \n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "    \n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Display the final score\n",
    "print(\"Final Score:\", score)\n",
    "\n",
    "# ä¿å­˜æ¸¸æˆè¿‡ç¨‹çš„å›¾åƒ\n",
    "for i, frame in enumerate(frames):\n",
    "    import imageio\n",
    "\n",
    "    # ä¿å­˜æ¸¸æˆè¿‡ç¨‹çš„å›¾åƒä¸ºGIFåŠ¨ç”»\n",
    "    imageio.mimsave('game_animation.gif', frames, fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
