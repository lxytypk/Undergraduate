{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a16982",
   "metadata": {},
   "source": [
    "### 1.测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35c977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9023, 0.9782, 0.5830],\n",
      "        [0.8467, 0.5440, 0.8336],\n",
      "        [0.1165, 0.3985, 0.6869],\n",
      "        [0.9527, 0.6089, 0.0115],\n",
      "        [0.8465, 0.4207, 0.0317]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd51cb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6eebe6",
   "metadata": {},
   "source": [
    "### 2.自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9c981",
   "metadata": {},
   "source": [
    "#### 2.1损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4624, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.ones(5)\n",
    "y=torch.zeros(3)\n",
    "w=torch.randn(5,3,requires_grad=True)\n",
    "b=torch.randn(3,requires_grad=True)\n",
    "z=torch.matmul(x,w)+b\n",
    "loss=torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fe4b3",
   "metadata": {},
   "source": [
    "#### 2.2反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ab284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_grad: None\n",
      "b_grad: None\n",
      "------------------------------\n",
      "w_grad After backward: tensor([[0.2616, 0.3008, 0.1358],\n",
      "        [0.2616, 0.3008, 0.1358],\n",
      "        [0.2616, 0.3008, 0.1358],\n",
      "        [0.2616, 0.3008, 0.1358],\n",
      "        [0.2616, 0.3008, 0.1358]])\n",
      "b_grad After backward: tensor([0.2616, 0.3008, 0.1358])\n"
     ]
    }
   ],
   "source": [
    "print('w_grad:',w.grad)\n",
    "print('b_grad:',b.grad)\n",
    "print('-'*30)\n",
    "loss.backward()\n",
    "print('w_grad After backward:',w.grad)\n",
    "print('b_grad After backward:',b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ff9d0",
   "metadata": {},
   "source": [
    "### 3.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01618140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([-0.1860], grad_fn=<AddBackward0>)\n",
      "------------------------------\n",
      "param: Parameter containing:\n",
      "tensor([[-0.0899,  0.1755]], requires_grad=True)\n",
      "param: Parameter containing:\n",
      "tensor([-0.4471], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "model=nn.Linear(2,1)\n",
    "input = torch.Tensor([1,2])\n",
    "output = model(input)\n",
    "print('output:',output)\n",
    "print('-'*30)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print('param:',param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91c64c",
   "metadata": {},
   "source": [
    "### 4.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011cba7",
   "metadata": {},
   "source": [
    "#### 4.1RNN参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d379d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: odict_keys(['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0'])\n",
      "torch.Size([10, 100]) torch.Size([10, 10])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "rnn=nn.RNN(100,10)\n",
    "print('parameters:',rnn._parameters.keys())\n",
    "\n",
    "# weight_ih_l0: 第0层的输入层和隐含层之间的权重\n",
    "# weight_hh_l0: 第0层的隐含层之间在不同时间步之间的权重\n",
    "print(rnn.weight_ih_l0.shape,rnn.weight_hh_l0.shape)\n",
    "\n",
    "print(rnn.bias_ih_l0.shape,rnn.bias_hh_l0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640a8c7",
   "metadata": {},
   "source": [
    "#### 4.2RNN前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be663ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 20])\n",
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "'''5层RNN'''\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# (词向量维度)feature_len=100, (神经元数)hidden_len=20, 网络层数=5\n",
    "rnn=nn.RNN(input_size=100,hidden_size=20,num_layers=5)\n",
    "# 单词数量(seq_len=10),句子数量(batch=3),每个特征100维度(feature_len=100)\n",
    "x=torch.randn(10,3,100)\n",
    "\n",
    "# h_0的shape是[网络层数=5, batch=3, (神经元数)hidden_len=20]\n",
    "out,h=rnn(x,torch.zeros(5,3,20))\n",
    "\n",
    "print(out.shape)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2633707",
   "metadata": {},
   "source": [
    "4层的RNN，用来做语音翻译，输入是一段中文，输出是一段英文。\n",
    "假设每个中文字符用100维数据进行编码，每个隐含层的维度是20，有4个隐含层。所以input_size = 100，hidden_size = 20，num_layers = 4。\n",
    "再假设模型已经训练好了，现在有个1个长度为10的句子做输入，那么seq_len = 10，batch_size =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7d0c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: RNN(100, 20, num_layers=4)\n",
      "ouput.shape: torch.Size([1])\n",
      "h.shape: torch.Size([4, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "rnn=nn.RNN(input_size=100,hidden_size=20,num_layers=4)\n",
    "print('rnn:',rnn)\n",
    "\n",
    "x=torch.randn(10,1,100)\n",
    "h0=torch.zeros(4,1,20)\n",
    "\n",
    "out,h=rnn(x,h0)\n",
    "print('ouput.shape:',output.shape)\n",
    "print('h.shape:',h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fb580",
   "metadata": {},
   "source": [
    "### 5.RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430db9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "'''单层RNN, feature_len=100, hidden_len=20'''\n",
    "cell1=nn.RNNCell(100,20)\n",
    "h1=torch.zeros(3,20)\n",
    "x=torch.zeros(10,3,100)\n",
    "for xt in x:\n",
    "    h1=cell1(xt,h1)\n",
    "print(h1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a084f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1.shape:  torch.Size([3, 30])\n",
      "h2.shape:  torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "'''多层RNN'''\n",
    "cell1=nn.RNNCell(100,30)\n",
    "cell2=nn.RNNCell(30,20)\n",
    "\n",
    "h1=torch.zeros(3,30)\n",
    "h2=torch.zeros(3,20)\n",
    "x=torch.rand(10,3,100)\n",
    "for xt in x:\n",
    "    h1=cell1(xt,h1)\n",
    "    h2=cell2(h1,h2)\n",
    "\n",
    "print('h1.shape: ',h1.shape)\n",
    "print('h2.shape: ',h2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416ac13",
   "metadata": {},
   "source": [
    "### 6.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996b5c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape:  torch.Size([10, 3, 20])\n",
      "h.shape:  torch.Size([4, 3, 20])\n",
      "c.shape:  torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#4层的LSTM,输入的每个词用100维向量表示,隐藏单元和记忆单元的尺寸是20\n",
    "lstm=nn.LSTM(input_size=100,hidden_size=20,num_layers=4)\n",
    "#3句话，每句10个单词，每个单词的词向量维度(长度)100\n",
    "x=torch.rand(10,3,100)\n",
    "#不传入h_0和c_0则会默认初始化\n",
    "out,(h,c)=lstm(x)\n",
    "\n",
    "print('out.shape: ',out.shape)\n",
    "print('h.shape: ',h.shape)\n",
    "print('c.shape: ',c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263aac6",
   "metadata": {},
   "source": [
    "### 7.LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec55d980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.shape:  torch.Size([3, 20])\n",
      "c.shape:  torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 单层LSTM\n",
    "# 1层的LSTM，输入的每个词用100维向量表示，隐藏单元和记忆单元的尺寸是20\n",
    "cell=nn.LSTMCell(input_size=100,hidden_size=20)\n",
    "# seq_len=10个时刻的输入,每个时刻shape都是[batch,feature_len]\n",
    "x=torch.randn(10,3,100)\n",
    "\n",
    "# 初始化隐藏单元h和记忆单元c,取batch=3\n",
    "h=torch.zeros(3,20)\n",
    "c=torch.zeros(3,20)\n",
    "\n",
    "# 对每个时刻,传入输入xt和上个时刻的h和c\n",
    "for xt in x:\n",
    "    b,c=cell(xt,(h,c))\n",
    "\n",
    "print('b.shape: ',b.shape)\n",
    "print('c.shape: ',c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fdfb5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 30]) torch.Size([3, 30])\n",
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 两层LSTM\n",
    "# 输入的feature_len=100,隐藏单元和记忆单元hidden_len=30\n",
    "cell1=nn.LSTMCell(input_size=100,hidden_size=30)\n",
    "# hidden_len从L0层的30变到这一层的20\n",
    "cell2=nn.LSTMCell(input_size=30,hidden_size=20)\n",
    "\n",
    "# 分别初始化L0层和L1层的隐藏单元h 和 记忆单元C,取batch=3\n",
    "h1=torch.zeros(3,30)\n",
    "C1=torch.zeros(3,30)\n",
    "\n",
    "h2=torch.zeros(3,20)\n",
    "C2=torch.zeros(3,20)\n",
    "x=torch.randn(10,3,100)\n",
    "\n",
    "for xt in x:\n",
    "    # 1层接受xt输入\n",
    "    h1,C1=cell1(xt,(h1,C1))\n",
    "    # 2层接受1层的输出h作为输入\n",
    "    h2,C2=cell2(h1,(h2,C2))\n",
    "\n",
    "print(h1.shape,C1.shape)\n",
    "print(h2.shape,C2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29918b5",
   "metadata": {},
   "source": [
    "### 8.词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d549759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5624, -1.2842,  0.7857,  0.6619,  1.3636]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#给单词编索引号\n",
    "word_to_idx={'hello':0,'world':1}\n",
    "#得到目标单词索引\n",
    "lookup_tensor=torch.tensor([word_to_idx['hello']],dtype=torch.long)\n",
    "#print(lookup_tensor)\n",
    "embeds=nn.Embedding(num_embeddings=2,embedding_dim=5)\n",
    "#传入单词的index，返回对应的嵌入向量\n",
    "hello_embed=embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac059b5",
   "metadata": {},
   "source": [
    "### 9.神经网络示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b3f48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        #定义一个卷积操作：1个图像通道, 6个输出通道, 5x5卷积\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:] #计算除通道以外的所有大小\n",
    "        # print(x.size())\n",
    "        # print(size)\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "    \n",
    "'''生成网络'''\n",
    "net=Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbfee7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "'''模型可训练的参数可以通过调用 net.parameters() 返回'''\n",
    "params=list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) #第一层卷积的权重系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d20f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0888,  0.0153,  0.0629, -0.0479, -0.0595,  0.1029,  0.0002, -0.0510,\n",
      "         -0.0515,  0.0367]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "'''随机生成一个 32x32 的输入'''\n",
    "input=torch.randn(1,1,32,32)\n",
    "out=net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44cc0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''把所有参数梯度缓存器置零，用随机的梯度来反向传播'''\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
