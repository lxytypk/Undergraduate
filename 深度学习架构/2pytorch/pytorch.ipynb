{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "'''单层RNN, feature_len=100, hidden_len=20'''\n",
    "cell1=nn.RNNCell(100,20)\n",
    "h1=torch.zeros(3,20)\n",
    "x=torch.zeros(10,3,100)\n",
    "for xt in x:\n",
    "    h1=cell1(xt,h1)\n",
    "print(h1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1.shape:  torch.Size([3, 30])\n",
      "h2.shape:  torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "'''多层RNN'''\n",
    "cell1=nn.RNNCell(100,30)\n",
    "cell2=nn.RNNCell(30,20)\n",
    "\n",
    "h1=torch.zeros(3,30)\n",
    "h2=torch.zeros(3,20)\n",
    "x=torch.rand(10,3,100)\n",
    "for xt in x:\n",
    "    h1=cell1(xt,h1)\n",
    "    h2=cell2(h1,h2)\n",
    "\n",
    "print('h1.shape: ',h1.shape)\n",
    "print('h2.shape: ',h2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape:  torch.Size([10, 3, 20])\n",
      "h.shape:  torch.Size([4, 3, 20])\n",
      "c.shape:  torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#4层的LSTM,输入的每个词用100维向量表示,隐藏单元和记忆单元的尺寸是20\n",
    "lstm=nn.LSTM(input_size=100,hidden_size=20,num_layers=4)\n",
    "#3句话，每句10个单词，每个单词的词向量维度(长度)100\n",
    "x=torch.rand(10,3,100)\n",
    "#不传入h_0和c_0则会默认初始化\n",
    "out,(h,c)=lstm(x)\n",
    "\n",
    "print('out.shape: ',out.shape)\n",
    "print('h.shape: ',h.shape)\n",
    "print('c.shape: ',c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.shape:  torch.Size([3, 20])\n",
      "c.shape:  torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 单层LSTM\n",
    "# 1层的LSTM，输入的每个词用100维向量表示，隐藏单元和记忆单元的尺寸是20\n",
    "cell=nn.LSTMCell(input_size=100,hidden_size=20)\n",
    "# seq_len=10个时刻的输入,每个时刻shape都是[batch,feature_len]\n",
    "x=torch.randn(10,3,100)\n",
    "\n",
    "# 初始化隐藏单元h和记忆单元c,取batch=3\n",
    "h=torch.zeros(3,20)\n",
    "c=torch.zeros(3,20)\n",
    "\n",
    "# 对每个时刻,传入输入xt和上个时刻的h和c\n",
    "for xt in x:\n",
    "    b,c=cell(xt,(h,c))\n",
    "\n",
    "print('b.shape: ',b.shape)\n",
    "print('c.shape: ',c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 30]) torch.Size([3, 30])\n",
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 两层LSTM\n",
    "# 输入的feature_len=100,隐藏单元和记忆单元hidden_len=30\n",
    "cell1=nn.LSTMCell(input_size=100,hidden_size=30)\n",
    "# hidden_len从L0层的30变到这一层的20\n",
    "cell2=nn.LSTMCell(input_size=30,hidden_size=20)\n",
    "\n",
    "# 分别初始化L0层和L1层的隐藏单元h 和 记忆单元C,取batch=3\n",
    "h1=torch.zeros(3,30)\n",
    "C1=torch.zeros(3,30)\n",
    "\n",
    "h2=torch.zeros(3,20)\n",
    "C2=torch.zeros(3,20)\n",
    "x=torch.randn(10,3,100)\n",
    "\n",
    "for xt in x:\n",
    "    # 1层接受xt输入\n",
    "    h1,C1=cell1(xt,(h1,C1))\n",
    "    # 2层接受1层的输出h作为输入\n",
    "    h2,C2=cell2(h1,(h2,C2))\n",
    "\n",
    "print(h1.shape,C1.shape)\n",
    "print(h2.shape,C2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[ 0.8669,  0.6445,  0.7020, -0.7155, -0.7310]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#给单词编索引号\n",
    "word_to_idx={'hello':0,'world':1}\n",
    "#得到目标单词索引\n",
    "lookup_tensor=torch.tensor([word_to_idx['hello']],dtype=torch.long)\n",
    "#print(lookup_tensor)\n",
    "embeds=nn.Embedding(num_embeddings=2,embedding_dim=5)\n",
    "#传入单词的index，返回对应的嵌入向量\n",
    "hello_embed=embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        #定义一个卷积操作：1个图像通道, 6个输出通道, 5x5卷积\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:] #计算除通道以外的所有大小\n",
    "        # print(x.size())\n",
    "        # print(size)\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "    \n",
    "'''生成网络'''\n",
    "net=Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "'''模型可训练的参数可以通过调用 net.parameters() 返回'''\n",
    "params=list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) #第一层卷积的权重系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1035, -0.0263,  0.1190, -0.0997, -0.0257, -0.0047, -0.0257, -0.0539,\n",
      "         -0.0380,  0.0274]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "'''随机生成一个 32x32 的输入'''\n",
    "input=torch.randn(1,1,32,32)\n",
    "out=net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''把所有参数梯度缓存器置零，用随机的梯度来反向传播'''\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
